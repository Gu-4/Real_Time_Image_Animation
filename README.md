# Real_Time_Image_Animation
This project uses a deep learning model to animate a still image using a driving video. 
It can simulate facial movements realistically and in real-time.
The system has potential applications in entertainment, virtual avatars, and also in deepfake detection and analysis, as it provides insight into how deepfakes are generated.

Input: A single image (e.g., Monalisa.png) and a short driving video (or GIF converted to MP4).
Model: The First Order Motion Model (FOMM) is used, which detects keypoints and transfers motion from video to the image.
Output: A realistic animated version of the input image mimicking the expressions and movements from the driving video.
